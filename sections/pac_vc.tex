% !TeX root = ../main.tex

\section{PAC-Learning and VC Dimension}
\subsection{PAC-Learning}
    Overfitting happens:
    \begin{itemize}
        \item \textbf{Because training error} is bad estimate of prediction error, not able to generalize (too much variance, unstable)
        \item \textbf{When learner} doesn't see "enough" examples, too less samples, how many samples are enough?
    \end{itemize}
    The setting:
    \begin{itemize}
        \item Set of instances $X$
        \item Set of hypothesis $H$ (finite)
        \item Set of possible target concepts $C$ (Boolean functions, in classification is true function)
        \item Training instances generated by a fixed unknown probability distribution $P$ over $X$
    \end{itemize}
    Learner \textbf{observes} sequence $D$ of training examples $\langle x,c(x)\rangle$ (input,output) for some target concept $c\in C$, where:
    \begin{itemize}
        \item Instances $x$ are drawn from distribution $P$
        \item Tracher provides \textbf{deterministic} target value $c(x)$ for each instance
    \end{itemize}
    Learner \textbf{must output a hypothesis} $h$ estimating $c$. $h$ is \textbf{evaluated} by its performance on \textbf{subsequent instances} drawn according to $P$
    $$L_{true}=Pr_{x\in P}[c(x)\neq h(x)]$$
    Which is a loss function, probability true target function $c(x)$ of my model $\neq$ my prediction $h(x)$, over all samples.\\
    We want to \textbf{bound} true error function through a function that contains train error:
    $$L_{true}\leq g(L_{train})$$
    \begin{itemize}
        \item True error = irreducible error+variance+bias$^2$
        \item Train error based on bias
        \item Variance based on \#samples, larger variance, larger hypothesis space (few samples, hypothesis decreases)
    \end{itemize}
\subsubsection{Version spaces and bad hypothesis}
    We would like to fall into a version space, which is a subset on the hypothesis space on which we have hypothesis with \textbf{zero train error, perfectly classifies} (but true error might not be zero).\\
    \textbf{How likely is learner to pick a bad hypothesis?} If the hypothesis space $H$ is \textbf{finite} and $D$ is a sequence $N\geq 1$ independent random examples of some target concept $c$, then for any $0\leq\epsilon\leq 1$, the probability that $VS_{H,D}$ contains a hypothesis error greater than $\epsilon$ is less than $|H|e^{-\epsilon N}$
    $$Pr(\exists\,\,h\in H\,:\,L_{train}(h)=0\,\,and\,\,L_{true}(h)\geq\epsilon)\leq|H|e^{-\epsilon N}$$
    $\epsilon$ is threshold on the error, \textbf{accuracy} for the hypothesis, \textbf{e.g. if = 0.1 it means probability that error higher than 0.1}\\
    Probability that hypothesis belongs to version space (train error = 0) and of very bad event occurring (true error bigger than threshold $\epsilon$) is bounded by that term that says if large $\epsilon$, low probability (same goes for $N$, prob decreases with \#samples).
\subsubsection{Proably approximately correct (PAC) bound}
    $\epsilon$ is the accuracy for the hypothesis. Let $\delta$ be the probability of bad event happening, if we want the previous probability to be at most $\delta$, I can find $N$ by picking $\epsilon$ and $\delta$:
    $$|H|e^{-\epsilon N}\leq\delta$$
    $$\vdots$$
    $$N\geq\frac{1}{\epsilon}\left(\ln|H|+\ln\frac{1}{\delta}\right)$$
    From which we could also compute $\epsilon$ by picking $N$ and $\delta$\\
    Number of $M$-ary boolean functions is $2^{2^M}$, so the bounds have \textbf{exponential} dependency on the number of features $M$\\
    Consider a class $C$ of possible target concepts defined over a set of instances $X$ of length $n$, and a Learner $L$ using hypothesis space $H$:
    \begin{itemize}
        \item $C$ is \textbf{PAC-learnable} if there exists an algorithm $L$ such that for every $c\in C$, for any distribution $P$, for any $\epsilon:0\leq\epsilon<1/2$ and $\delta:0\leq\delta<1/2$, $L$ with probability at least $1-\delta$ outputs a concept $h$ such that $L_{true}(h)\leq\epsilon$ using a number of samples that is polynomial of $1/\epsilon$ and $1/\delta$
        \item $C$ is \textbf{efficiently PAC-learnable} by $L$ using $H$ iff for all $c\in C$, distributions $P$ over $X$, $\epsilon:0\leq\epsilon<1/2$ and $\delta:0\leq\delta<1/2$, $L$ with probability at least $1-\delta$ will output a hypothesis $h\in H$ such that $L_{true}(h)\leq\epsilon$, in time that is \textbf{polynomial} in $1/\epsilon$, $1/\delta$, $M$ and $size(c)$
    \end{itemize}
\subsubsection{Agnostic learning}
    Till now we say that there exists a version space, region in which hypothesis with zero train error, but what if best train error is bigger than zero?
    \begin{itemize}
        \item We need to bound the gap between training and true errors:
        $$L_{true}(h)\leq L_{train}(h)+\epsilon$$
        \item Using \textbf{Hoeffding bound}, we obtain:
        $$Pr(\mathbb{E}[\overline{X}]-\overline{X}>\epsilon)<e^{-2N\epsilon^2}$$
        Then similarly to before, hypothesis space $H$ finite, dataset $D$ with $N$ i.i.d. samples, $0<\epsilon<1$: for any learner hypothesis $h$:
        $$Pr(\exists\,\,h\,\in H|L_{true}(h)-L_{train}(h)>\epsilon)\leq|H|e^{-2N\epsilon^2}$$
    \end{itemize}
\subsubsection{PAC bound and bias-variance tradeoff}
    Larger hypothesis space, more prediction error, more variance
    $$L_{true}=L_{train}+\epsilon\leq\underset{Bias}{\underbrace{L_{train}(h)}}+\underset{Variance}{\underbrace{\sqrt{\frac{\ln|H|+\ln\frac{1}{\delta}}{2N}}}}$$
    \begin{itemize}
        \item For large |H|
        \begin{itemize}
            \item Low bias (assuming we can find a good $h$)
            \item High variance (because bound is looser)
        \end{itemize}
        \item For small |H|
        \begin{itemize}
            \item High bias (is there a good $h$?)
            \item Low variance (tighter bound)
        \end{itemize}
        \item Given $\delta$, $\epsilon$, $N$ should be:
        $$N\geq\frac{1}{2\epsilon^2}\left(\ln|H|+\ln\frac{1}{\delta}\right)$$
    \end{itemize}

\subsection{VC Dimension}
    The VC dimension is used to compute a bound between the training error and the "true" error, in cases in which we have an infinite hypothesis space. A simple example of the latter is for example the linear classifier.
    \begin{itemize}
        \item A \textbf{dichotomy} of a set $S$ is partition of $S$ into two disjoint subsets (e.g. any partition of input samples)
        \item A set of instances $S$ is \textbf{shattered} by hypothesis space $H$ if and only if for every dichotomy of $S$ there exists some hypothesis in $H$ consistent with this dichotomy ($\forall$ dichotomy $\exists$ $h$ that perfectly classifies it)
    \end{itemize}
    The \textbf{Vapnic-Chervonenkis dimension}, $VC(H)$ of hypothesis space $H$ defined over instance space $X$ is the \textbf{size of the largest finite subset} of $X$ shattered by $H$. If arbitrarily large finite sets of $X$ can be shattered by H, then $VC(H)=\infty$.\\
    \textbf{A linear boundary classifies exactly M+1 points in M-Dimension}.
    \begin{itemize}
        \item How many randomly drawn examples suffice to guarantee error of at most $\epsilon$ with probability at least $(1-\delta)$?
        $$N\geq \frac{1}{\epsilon}\left(4\log_2\left(\frac{2}{\delta}\right)+8VC(H)\log_2\left(\frac{13}{\epsilon}\right)\right)$$
        \item Bound the true error:
        $$L_{true}(h)\leq L_{train}(h)+\sqrt{\frac{
            VC(H)\left(\ln\frac{2N}{VC(H)}+1\right)+\ln\frac{4}{\delta}
        }{N}}$$
        Same bias-variance tradeoff as always (first term of train error is bias, second of sqrt is variance), now just a function of $V(H)$.\\
        \textbf{Structural risk minimization}, choose $H$ to minimize the above bound on expected true error (insted of using LOO)
    \end{itemize}
    VC dimension properties:
    \begin{itemize}
        \item The VC dimension of a hypothesis space $|H|<\infty$ is bounded from above
        $$VC(H)\leq\log_2(|H|)$$
        \item Concept class $C$ with $VC(C)=\infty$ is not PAC-learnable
        \item \textbf{High VC dimension, more complex model, larger error}
        \item \textbf{Does not depend in \#samples/size of training or test set}
    \end{itemize}