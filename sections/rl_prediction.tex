% !TeX root = ../main.tex

\section{RL: Model-Free Prediction}
\subsection{Reinforcement Learning}
\begin{itemize}
    \item \textbf{Model-free vs Model-based}, in model-based we estimate model from data
    \item \textbf{On-policy vs Off-policy}, on-policy learn value function from policy, off-policy we want to estimate performance of a different policy, observe behavior of $\overline{\pi}$ and then estimate on something else like $\pi$
    \item \textbf{Online vs Offline}, online updates policy while we gather data, offline collects data and given the data we compute the solution offline, without interacting anymore
    \item \textbf{Tabular vs Function approximation}, tabular learn value utility of each state-action in the table, cannot be 	done if we have continuous states (in this case function approximation, but not that easy as in supervised, in which we have the input)
    \item \textbf{Value-based vs Policy-based vs Actor-critic}
    \begin{itemize}
        \item Value-based observes performance and changes policy, exploits a lot the Markovian structure of the problem, but not very good in continuous state and action
        \item Policy-based follows/computes gradient, works well in continuous
        \item Actor-critic, combination
    \end{itemize}
    \item Can \textbf{manage continuous space}, with infinite \#states we can use discretization or functional approach, function approximating my estimator
\end{itemize}
For model-free, two kinds of RL problems:
\begin{tabularx}{\linewidth}{X X X}
    \toprule
    \textbf{Problem} & \textbf{Description} & \textbf{Algorithms}\\
    \midrule
    \endfirsthead
    \toprule
    \textbf{Problem} & \textbf{Description} & \textbf{Algorithms}\\
    \midrule
    \endhead
    \midrule
    \footnotesize [Continues on next page]
    \endfoot
    \bottomrule
    \endlastfoot
    \multirow{3}{*}{\textbf{Model-free Prediction}} & Estimates the value function of an unknown MRP (MDP+policy), \textbf{fixed policy}, we want to know how much we get from the environment (cannot say anything about optimal)& Monte-Carlo (first and every visit) \\ \cmidrule{3-3}
    & & Temporal Difference \\ \cmidrule{3-3}
    & & TD($\lambda$) (forward and backward-view)\\ \midrule
    \multirow{2}{*}{\textbf{Model-free Control}} & Optimizes the value function of an unknown MDP, \textbf{actively change policy} to get optimal policy & On-policy Monte-Carlo $\epsilon$-greedy (GLIE) \\ \cmidrule{3-3}
    & & On-policy Temporal Difference $\epsilon$-greedy (SARSA) \\ \cmidrule{3-3}
    & & Off-policy Importance Sampling (off-policy Monte-Carlo and SARSA) \\ \cmidrule{3-3}
    & & Off-policy Q-learning
\end{tabularx}
    In MDP prediction solved with matrix inversion (closed-form analytical solution) or Bellman and control solved with value and policy iteration

\subsection{Monte-Carlo}
\subsubsection{Main Points}
    \begin{itemize}
        \item Episodic problem, episode is a situation in which sooner or later reaches end, a set of data that provides the dynamic of the system, state, action and reward
        \item \textbf{All episodes must terminate}, otherwise cannot apply (works only on episodic MDPs)
        \item Learns directly from episodes of experience (\textbf{use historical data}), from \textbf{complete} episodes, \textbf{no bootstrapping, but sampling also entire episode included}
        \item Model-free, no knowledge of MDP transitions/rewards
        \item Uses simple idea, value=mean return
        \item Only one at each state (unlike DP)
        \item Time required to estimate one state does not depend on the total number of states
        \item Stochasticity in the rewards requires the use of a \textbf{larger number of episodes} to have \textbf{precise prediction} of the MDP value in the case we use MC estimation
        \item Works better (than TD) if the problem \textbf{is not Markovian}, does not use any information about the transition model of the MDP, thus no assumption on the transition model is considered when using MC
    \end{itemize}
    Can be used both for prediction or control:
    \begin{itemize}
        \item \textbf{Prediction} input are episodes of experience generated by following policy $\pi$ in given MDP or episodes of experience generated by MRP
        $$\left\{s_1,a_1,r_2,\cdots,s_T\right\}\rightarrow V^\pi$$
        \item \textbf{Control} input are episodes of experience in given MDP
        $$\left\{s_1,a_1,r_2,\cdots,s_T\right\}\rightarrow (V^*,\pi^*)$$
    \end{itemize}
    Monte-Carlo works by using the expected \textbf{empirical mean} of the returns from each state instead of the expect return to estimate the state-value function. We collect various episodes and compute the empirical mean of the returns for each to estimate $V^\pi(s)$.
\subsubsection{First and every visit}
    Reminder: the return is total discounted reward
    $$v_t=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-1}r_{t+T}$$
    And the \textbf{state-value} function $V^{\pi}(s)$ of an MDP is the expected \textbf{return} starting from state s and following policy $\pi$
    $$V^{\pi}(s)=\mathbb{E}_{\pi}[v_t|s_t=s]$$
    By using the empirical mean instead of the expected return:
    \begin{itemize}
        \item \textbf{First visit}: average returns only for the first time $s$ is visited (\textbf{unbiased estimator}), one sample for each state, each state visited only once
        \begin{itemize}
            \item \textbf{Independent from samples, large variance}. Use it with large data, in which we don't care about variance
            \item \textbf{Does not depend on length of the episode}, at most \#samples=\#states
        \end{itemize}
        \item \textbf{Every visit}: average returns for every time $s$ is visited (\textbf{biased estimator, uses biased state values but consistent}), multiple samples for each visit
        \begin{itemize}
            \item \textbf{Dependent on samples, smaller variance}. Use it with small data, in which we care about variance but more biased better$\Rightarrow$with small amount of episodes/few samples we reduce the variance
            \item \textbf{Depends on length of the episode}, we have a sample per states in the episode
        \end{itemize}
    \end{itemize}
\subsubsection{Incremental MC updates}
    We can also update $V(s)$ \textbf{incrementally} after episode $s_1,a_1,r_2,\cdots,s_T$: for each state $s_t$ with return $v_t$
    $$N(s_t)\leftarrow N(s_t)+1$$
    $$V(s_t)\leftarrow V(s_t)+\frac{1}{N(s_t)}(v_t-V(s_t))$$
    Where $N(s_t)$ is the number of times we have traversed that state.\\
    In \textbf{non-stationary} problems it is useful to track a running mean, i.e., forget old episodes:
    $$V(s_t)\leftarrow V(s_t)+\alpha (v_t-V(s_t))$$
    \textbf{Incremental every visit MC}
    If the learning rate $\alpha > \frac{1}{N}$ we will give more weight to the recent sample.

\subsection{Temporal Difference}
\subsubsection{Main Points}
    \begin{itemize}
        \item Learns step-by-step, while in MC waits for the end of the episode, here one step with real reward and the followings are guess
        \item \textbf{Online} method somehow, processes data as they come, \textbf{empirical version of Bellman expectation equation}
        \item Uses Bellman equation and instantiates for one sample, no need to wait
        \item \textbf{Bootstraps}, learns from \textbf{incomplete episodes}
        \item \textbf{Consistent}, this means it will converge to the exact evaluation of the policy taking the data, which can be different from the optimal policy, if we consider an infinite number of transitions it becomes \textbf{unbiased}
        \item Can be used when there is \textbf{no terminal state in the original MDP}, updates are performed at each transition, does not need to have finite episodes or wait
        \item \textbf{Less variance} than MC, \textbf{some bias}
    \end{itemize}
\subsubsection{TD(0)}
    Learns $V^\pi$ \textbf{online} from experience under policy $\pi$.\\
    Recall: incremental every visit MC:
    $$V(s_t)\leftarrow V(s_t)+\alpha (v_t-V(s_t))$$
    Simplest temporal-difference learning algorithm is TD(0), update value $V(s_t)$ towards \textbf{estimated return/TD target} $r_{t+1}+\gamma V(s_{t+1})$:
    $$V(s_t)\leftarrow V(s_t)+\alpha (r_{t+1}+\gamma V(s_{t+1})-V(s_t))$$
    Where
    \begin{itemize}
        \item $r_{t+1}+\gamma V(s_{t+1})$ is called the \textbf{TD target}
        \item $\delta_t=r_{t+1}+\gamma V(s_{t+1})-V(s_t)$ is called the \textbf{TD error}, what I learned so far and what is the new info
        \item \textbf{Biased estimator}, uses \textbf{dependent samples} $\alpha$ introduces a lot of bias, but reduces over time: like Bellman operator each value will reach its fixed point
        \item The TD estimator $V(s_t)\leftarrow V(s_t)+\alpha (r_{t+1}+\gamma V(s_{t+1})-V(s_t))$ \textbf{has less variance} as in MC we are not reducing variance as sum all rewards, in TD we average rewards/values (\textbf{averaging more transitions}): return depends on many random actions, transitions, rewards while TD target depends on \textbf{one} random action, transition, reward
    \end{itemize}
\subsubsection{MC vs TD}
    \begin{tabularx}{\linewidth}{X|X|X}
        \toprule
        \textbf{Property} & \textbf{Monte-Carlo} & \textbf{Temporal Difference}\\
        \midrule
        \endfirsthead
        \toprule
        \textbf{Property} & \textbf{Monte-Carlo} & \textbf{Temporal Difference}\\
        \midrule
        \endhead
        \footnotesize [Continues on next page]
        \endfoot
        \bottomrule
        \endlastfoot
        %body
        \multirow{2}{*}{\textbf{Learning}} & Must wait end of episode before return is known & Can learn before knowing final outcome, learns \textbf{online (uses bootstrapping)}, after every step updates the value function after each instantaneous reward\\ \cmidrule{2-3}
        & Can learn only from complete sequences & Can learn from incomplete sequences\\ \midrule
        \textbf{Environment} & Works only for episodic (terminating) environments or \textbf{indefinite horizon ML}, use episodes with different length & Works in \textbf{continuing (non-terminating, infinite horizon)} environments or \textbf{indefinite horizon ML}, might be updated after each transition \\ \midrule
        \textbf{Bias} & Zero bias on FV, some bias on EV& Some bias, uses \textbf{dependent samples}, but reduces over time \\ \midrule
        \textbf{Variance} & High variance & Low Variance (so with few interactions where variance main issue, TD is better)\\ \midrule
        \textbf{Convergence} & Good converge properties, simpler to use and understand & More efficient, TD(0) converges to $V^\pi(s)$ \\ \midrule
        \textbf{Function approximation} & Works well & Problems \\ \midrule
        \textbf{Initial value sensitivity} & Not very sensitive & More sensitive \\ \midrule
        \textbf{Bootstrapping} & No, yes with $\alpha\neq\frac{1}{n}$ in its incremental version, $n$ being the number of states processed so far & Yes, it computes the estimates by using the information learned so far\\ \midrule
        \textbf{Sampling} & Yes & Yes \\ \midrule
        \textbf{\#samples and \#states} & FV \#samples \textbf{depends} on dimension (\#states) of the MDP, you will get only one sample per state in each episode assuming to have fewer states than transitions of the episodes & \#states \textbf{does not matter}, for each transition I extracat a sample, each time I have transition I update estimate \\ \midrule
        \textbf{\#samples depends on len(episode)} & EV Yes, one sample per states in the episode & Yes \\ \midrule
        \textbf{Reuse information learned from past learning steps} & No, yes when bootstrapping with $\alpha\neq\frac{1}{n}$ & TD since bootstraps\\ \midrule
        \textbf{Make use of the Markov property of MDP} & No, \textbf{works better if not Markovian}, does not use any information about the transition model of the MDP & Yes, it explicitly uses it to compute the state-value function
    \end{tabularx}

\subsection{TD($\lambda$)}
    $\lambda$ in 0,1, 0 is TD, 1 is MC, let the TD look $n$ steps into the future, the return:\\
    $n=1$ \hspace{2em}(TD)\hspace{2em} $v_t^{(1)}=r_{t+1}+\gamma V(s_{t+1})$\\
    $n=2$ \hspace{6em} $v_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2V(s_{t+1})$\\
    $\vdots$\\
    $n=\infty$\hspace{2em}(MC)\hspace{2em}$v_t^{\infty}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{T-1}r_{T}$\\
    \textbf{Larger n, more variance and less bias}\\
    The $n$-step return and temporal difference learning:
    $$v_t^{(n)}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{n-1}r_{t+n}+\gamma^nV(s_{t+n})$$
    $$V(s_t)\leftarrow V(s_t)+\alpha(v_t^{(n)}-V(s_t))$$
    Takes into account \textbf{not only last sample saw, but also previous ones, but discounted}
\subsubsection{Forward-view}
    Averaging n-steps returns and $\lambda$-return:
    \begin{itemize}
        \item The $\lambda$-return $v_t^\lambda$ combines all $n$-step returns $v_t^{(n)}$
        \item Using weight $(1-\lambda)\lambda^{n-1}$
        $$v_t^\lambda =(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}v_t^{(n)}$$
        \item  Forward-view TD($\lambda$)
        $$V(s_t)\leftarrow V(s_t)+\alpha(v_t^\lambda-V(s_t))$$
    \end{itemize}
    For $\lambda=0$/TD(0) we have 1-step, TD\\
    For $\lambda=1$/TD(1) we have MC, but in reality approximation of MC EV\\
    Forward-view: update state $s_t$ by averaging future rewards (not step by step)
\subsubsection{Backward-view}
    How to propagate reward back?
    \begin{itemize}
        \item \textbf{Frequency heuristic}: assign credit to the most frequent states
        \item \textbf{Recency heuristic}: assign credit to the most recent states
        \item \textbf{Eligibility traces}: combine both heuristics
        $$e_{t+1}(s)=\gamma \lambda e_t(s)+1(s=s_t)$$
        For each state keep a value: eligibility of the state. Increase by 1 and reduce by $\gamma \lambda$.\\
        If $\gamma=0$ only one state is eligible.\\
        Larger $\lambda$, larger trace I'm leaving behind.\\
        \textbf{After each step, update all states with eligibility trace > 0}.
    \end{itemize}
    \begin{itemize}
        \item Update value $V(s)$ for every state $s$
        \item In proportion to TD-error $\delta_t$ and eligibility trace $e_t(s)$\\
        $e_0(s)=0$\\
        $e_t(s)=\gamma \lambda e_{t-1}(s)+1(s=s_t)$\\
        $V(s)\leftarrow V(s)+\alpha \delta_te_t(s)$
        \item The update rule same as TD update rule, applies to eligible states
    \end{itemize}
    \fbox{\begin{minipage}{\linewidth}
        \begin{addmargin}[0em]{0em}
            Initialize $V(s)$ arbitrarily\\
            \textbf{for all} episodes \textbf{do}
            \begin{addmargin}[1em]{0em}
                $e(s)=0$, $\forall s \in S$\\
                Initialize $s$\\
                \textbf{repeat}
                \begin{addmargin}[1em]{0em}
                    $a\leftarrow$action given by $\pi$ for $s$\\
                    Take action $a$, observe reward $r$, and next state $s'$\\
                    $\delta\leftarrow r+\gamma V(s')-V(s)$\\
                    $e(s)\leftarrow e(s)+1$\\
                    \textbf{for all} $s \in S$ \textbf{do}
                    \begin{addmargin}[1.5em]{0em}
                        $V(s)\leftarrow V(s)+\alpha\delta e(s)$\\
                        $e(s)\leftarrow \gamma\lambda e(s)$
                    \end{addmargin}
                    \textbf{end for}\\
                    $s\leftarrow s'$
                \end{addmargin}
                \textbf{until} $s$ is terminal
            \end{addmargin}
            \textbf{end for}
        \end{addmargin}
    \end{minipage}}

        
        