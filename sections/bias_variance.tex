% !TeX root = ../main.tex

\section{Bias-Variance Tradeoff}
\subsection{"No Free Lunch" Theorems}
    No free lunch theorems for classification, in particular binary classification, \textbf{accuracy of learner $\rightarrow$ generalization capability of learner (learner = techniques like regression tree, neural networks...)}.\\
    $Acc_G(L)=$\textbf{Generalization accuracy} of learner L, accuracy of L on \textbf{non-training} examples\\
    $F=$set of all possible concepts, all possible functions, in classification all possible partitions, $y=f(x)$

    \vspace{1em}
    \textbf{Average accuracy of learner given set of all possible functions}:\\
    $$\frac{1}{|F|}\sum_FAcc_G(L)=\frac{1}{2}$$
    Given any distribution $P$ over $x$ and training set size $N$, any learner cannot be better than random guessing, set of all possible function, \textbf{if everything can happen cannot learn, must deduct from similarities}.

    \vspace{1em}
    When using ML we assume that \textbf{there is a compression in the possibilities, which leads to regularities that can be leveraged for predicting}. $\frac{1}{|F|}$ means that any function can be true function.

    \vspace{1em}
    \textbf{Corollary}:\\
    For any two learners $L_1,L_2$, if $\exists$ learning problem s.t. $Acc_G(L_1)>Acc_G(L_2)$ then $\exists$ learning problem s.t. $Acc_G(L_2)>Acc_G(L_1)$

    \vspace{1em}
    \textbf{It is always possible to find a set of data where an algorithm performs arbitrarily bad}, therefore \textbf{on average} it is not able to beat a random guess.\\
    According to the No Free Lunch theorem we can expect all the learning algorithms
    to perform equally bad on the average over the concepts. Instead, the specific structure of a given concept can favor an algorithm over another.\\
    Don't expect that your favorite learner to always be the best, try different approaches and compare: \textbf{given a specific task we are able to find an algorithm which is likely to perform better than a random guess}.\\But how could (say) a deep neural network be less accurate than a single-layer one? More complex does not mean better.\\How to choose among the different techniques in a correct way?
\subsection{Bias and Variance}
    We focuse on regression:
    \begin{itemize}
        \item Assume target values generated by a function $f$ with some noise $\epsilon$: $t_i=f(x_i)+\epsilon$
        \item Zero mean and fixed variance: $\mathbb{E}[\epsilon]=0,\,Var[\epsilon]=\sigma^2$
        \item We want to find a model $y(x)$ that approximates f as well as possible
    \end{itemize}
    We consider the square error, least square and decompose it:
    $$\mathbb{E}[(t-y(x))^2]=\cdots=\underset{\sigma^2}{\underbrace{Var[t]}}+\underset{Variance}{\underbrace{Var[y(x)]}}+\underset{Bias^2}{\underbrace{\mathbb{E}[f(x)-y(x)]^2}}$$
    \begin{itemize}
        \item $t$ is stochastic, sum of stochastic part and noise
        \item $y$ is stochastic, dataset, depends on random variable
        \item $\mathbb{E}[t]=f(x)$, $f$ is deterministic, has zero variance
        \item $\sigma^2$ is \textbf{irreducible error, related to noise of the data}
        \item $Bias^2$ is sort of error factor
        \item \textbf{This is our loss function to minimize, can operate on variance and bias}
    \end{itemize}
    \begin{tabularx}{\linewidth}{X X X}
        \toprule
        \textbf{} & \textbf{Bias} & \textbf{Variance}\\
        \midrule
        \endfirsthead
        \toprule
        \textbf{} & \textbf{Bias} & \textbf{Variance}\\
        \\
        \midrule
        \endhead
        \midrule
        \footnotesize [Continues on next page]
        \endfoot
        \bottomrule
        \endlastfoot
        \multirow{2}{*}{\textbf{Definition}} & Difference between the truth and what you expect to learn & Difference between what you learn from a particular dataset and what you expect to learn\\ \cmidrule{2-3}
        & $\int(f(x)-\mathbb{E}[y(x)])^2p(x)dx$ & $\int\mathbb{E}[y(x)-\mathbb{E}[y(x)]^2]p(x)dx$\\[1ex] \midrule
        \textbf{Reduction} & \multirow{3}{14em}{Increase hypothesis space, more complex model} & \multirow{3}{14em}{Reduce hypothesis space, simpler models}\\[5ex] \cmidrule{2-3}
        & Regularize less (reduce $\lambda$ in ridge or lasso) & Regularize more\\ \cmidrule{3-3}
        & & Increase samples
    \end{tabularx}
    Hypothesis space large or small w.r.t. number of samples (10 samples, 10 parameters, hypothesis space too large).
\subsubsection{Training, prediction and test}
    \begin{itemize}
        \item \textbf{Training error}: what we minimize in OLS for example, overfitting problem as we choose hypothesis space as large as possible -> high variance.\\
        Minimizing training error leads to overfitting, we choose hypothesis space as large as possible.
        \begin{itemize}
            \item \textbf{Increase model complexity (more parameters)$\Rightarrow$more variance (overfitting)$\Rightarrow$Less training error}
            \item \textbf{Reduce training error$\Rightarrow$more variance (overfitting)}
            \item \textbf{Regularizing more$\Rightarrow$less variance$\Rightarrow$more training error}
            \item \textbf{Fix model complexity(fixed bias)$\Rightarrow$more samples$\Rightarrow$less variance$\Rightarrow$more training error}
        \end{itemize}
        \item \textbf{Prediction/true error}: impossible to compute the true error, also if we reduce training error, prediction error will increase at a certain point: as in true error we have also variance, so if variance increases, prediction error will increase too.\\
        Training error is an optimistically biased estimate of the prediction error.
        \begin{itemize}
            \item \textbf{Reduce training error$\Rightarrow$more variance$\Rightarrow$more prediction error}
            \item \textbf{Less variance$\Rightarrow$less prediction error}
        \end{itemize}
        \item \textbf{Test error}: divide data set \textbf{randomly} into test and training data:
        \begin{itemize}
            \item \textbf{Training data: optimize parameters}, determines goodness of model and size of hypothesis space, learns the model parameters
            \item \textbf{Test data: evaluate prediction error}, test performance of our model, determines accuracy, \textbf{unbiased if not used for learning (including parameters selection)}
        \end{itemize}
        \textbf{Worse test error if model too complex/overfitting}.\\
        \textbf{If test and train error comparable, high bias, far from desired performance, under-fitting}.\\
        How to divide them?\\
        \#testing < \#training $\rightarrow$ more uncertainty\\
        \#testing > \#training $\rightarrow$ less uncertainty, smaller confidence interval but poor model
    \end{itemize}
\subsubsection{To manage the tradeoff}
    \begin{itemize}
        \item \textbf{Model selection}
        \begin{itemize}
            \item \textbf{Feature selection}, identifies a subset of input features that are most related to the output ($n$ features, $2^n$ subsets, true/false)
            \begin{itemize}
                \item Validation
                \item LOO
                \item k-fold cross validation
            \end{itemize}
            In practice:
            \begin{itemize}
                \item Filter methods
                \item Embedded FS
                \item Wrapper methods
                \begin{itemize}
                    \item Brute force
                    \item Forward step-wise selection
                    \item Backward step-wise selection
                \end{itemize}
            \end{itemize}
            \item \textbf{Regularization}, all the input features are used, but the estimated coefficients are shrunked towards zero, thus reducing the variance
            \begin{itemize}
                \item Ridge regression
                \item Lasso
                \item Elastic net
                \item K-nearest neighbour
            \end{itemize}
            \item \textbf{Dimensionality reduction (unsupervised learning)}, the input variables are projected into a lower-dimensional subspace
            \begin{itemize}
                \item Principal component analysis (PCA)
            \end{itemize}
        \end{itemize}
        \item \textbf{Model ensemble}
        \begin{itemize}
            \item \textbf{Bagging/bootstrapping}
            \item \textbf{Boosting}
        \end{itemize}
    \end{itemize}
\subsection{Model Selection}
    Why want to reduce dimension of input space? \textbf{Curse of dimensionality}, working with high-dimensional data is difficult:
    \begin{itemize}
        \item Large \textbf{variance, overfitting}
        \item Needs \textbf{many samples}: $N^d$
        \item High \textbf{computational cost}
    \end{itemize}
    Model selection to compress input space
\subsubsection{Feature selection}
    Identifies a subset of input features that are most related to the output ($n$ features, $2^n$ subsets, true/false).\\
    Suppose we could compute all possible subsets, what is the best subset of features? We have to compare the errors, but we cannot use the training set nor the test set (selecting best set is learning process, cannot use dataset): we need another set of data \textbf{validation set used to select the model, but which dataset to reduce? We use methods that require more computational power that saves some data from training set}.\\
    \textbf{To find the best subset}:
    \begin{itemize}
        \item \textbf{Validation}
        \begin{itemize}
            \item \textbf{Dataset: large}, if dataset small means a large portion kept for training
            \item \textbf{Model: complex}, we want to train it once
        \end{itemize}
        \item \textbf{Leave-one-out cross validation (LOO)}
        \begin{itemize}
            \item \textbf{Dataset: small}
            \item \textbf{Model: simple}
            \item Samples dataset $D$, training with $D\setminus\{n\}$, training data with the $n$-th data point moved to validation point
            \item Estimation error based on the performance over only one point
            \item Train $|D|$ models, average over all data points:
            $$L_{LOO}=\frac{1}{N}\sum_{n=1}^N(t_n-y_{D\setminus\{n\}}(x_n))^2$$
            \item \textbf{Almost unbiased, slightly pessimistic estimates of the test error, provides slightly larger estimates of the prediction error on newly seen data}
            \item \textbf{Robust average, but computationally unfeasible, use with parallel computing abilities since each training procedure independent from another}
        \end{itemize}
        \item \textbf{k-fold cross validation}
        \begin{itemize}
            \item \textbf{Dataset: large}
            \item \textbf{Model: simple}
            \item Generalization of LOO, instead of 1 sample, 1 fold (group of samples)
            \item \textbf{Bigger k$\Rightarrow$more accuracy, more computational power required} (k=$|D|$ is LOO)
            \item \textbf{Smaller k$\Rightarrow$less accuracy, less computational power required}
        \end{itemize}
        \textbf{Randomly} divide training data into $k$ equal parts $D_1,\cdots,D_k$, for each $i$:
        \begin{itemize}
            \item Learn model $y_{D\setminus D_i}$ using data points not in $D_i$
            \item Estimate error of $y_{D\setminus D_i}$ on validation set $D_i$
            $$L_{D_i}=\frac{k}{N}\sum_{(x_n,t_n)\in D_i}(t_n-y_{D\setminus D_i}(x_n))^2$$
            \item $k$-fold cross validation error is average over data splits
            $$L_{k-fold}=\frac{1}{k}\sum_{i=1}^k L_{D_i}$$
            \item \textbf{Much faster to compute than LOO}
            \item \textbf{More (pessimistically) biased}
            \item Usually $k=10$
        \end{itemize}
    \end{itemize}
    After finding the best subset of features, take all data both from training and validation and train this best model, then testing with test set.\\
    If performance poor cannot retry, try new test set.\\
    \textbf{Feature selection in practice, metaheuristics based on cross validation to build models}:
    \begin{enumerate}
        \item \textbf{Filter}: rank the features (ignoring the classifier) and select the best ones, but there might be features that are highly correlated to each other, so few new information (e.g. Lasso). \textbf{Ranks once, then trains, don't know performance of final model, assumes features are independent}
        \item \textbf{Embedded} (Built-in): method used for supervised learning that intrinsically selects features, the learning algorithm exploits its own variable selection technique, e.g. Lasso (penalizes large weights), Decision trees, Auto-encoding, etc
        \item \textbf{Wrapper}: use heuristics on selection of subsets, evaluate only some subsets. \textbf{Solves optimization problem, requires training several models}
        \begin{itemize}
            \item \textbf{Brute force}: for each feature $x_k$ with $k\in\{1,\cdots,M\}$, learn all the possible $\begin{pmatrix}
                M \\ k
            \end{pmatrix}$ models with $k$ inputs and select the model with the smallest lost (and the $k$ providing that model).\\
            If M too large, computation of all models is unfeasible.
            \item \textbf{Forward selection}: starts from an empty model and adds features one-at-a-time, until you see no more improvement in adding more features.\\
            Start training models, each one has a single feature. Evaluate the \textbf{performance} of each model, then take the best one, then go on considering couples including the first feature. Continue till performance starts decreasing (I DON'T WANT TO TAKE ALL FEATURES).\\
            Worst case: take all features, which means I trained $M+(M-1)+(M-2)\cdots\sim M^2$ models
            \item \textbf{Backward Elimination}: starts with all the features and removes the least useful feature, one-at-a-time. Check the \textbf{accuracy on validation}, test set should not be included in the validation process, training error will increase as long as we increase \#variables, larger error less accuracy. Also if model selection performed on training accuracy, we would always select the model that keeps all the features
        \end{itemize}
    \end{enumerate}
    \textbf{Adjustment techniques}, the higher the k in cross validation, the better estimation of best model, but requires a lot of computational power.\\
    The computation of these indexes does not require to retrain the model, is computed over the training data and automatically penalizes complex models.\\
    \textbf{Small dataset, complex model}.\\
    We could use a fixed validation set, but this reduces training data. \textbf{If no enough computational power use adjustment techniques, approximation of prediction error using training error. If model complexity low (relatively to size of hypothesis space), train and prediction error almost same.\\
    Small values, small test error}:
    \begin{itemize}
        \item $C_p=\frac{1}{N}(RSS+2d\tilde{\sigma}^2)$\\
        Where $d$ is the total number of parameters (higher it is, higher the correction and lower RSS), $\tilde{\sigma}^2$ is an estimate of the variance of the noise $\epsilon$, $RSS$ is the train error. 
        \item $AIC=-2\log L+2d$\\
        Where $L$ is the maximized value of the likelihood function for the estimated model.
        \item $BIC=\frac{1}{N}(RSS+\log(N)d\tilde{\sigma}^2)$\\
        Similar to $C_p$ but different second term: since $\log N > 2$ for $n>7$, BIC selects smaller models
        \item $AdjustedR^2=1-\frac{RSS/(N-d-1)}{TSS/(N-1)}$\\
        Where $TSS$ is the total sum of squares. Differently from other criteria, \textbf{a large value indicates model with small test error}.
    \end{itemize}
\subsubsection{Regularization}
    All the input features are used, but the estimated coefficients are shrunked towards zero, thus reducing the variance.\\
    If the model complexity increases (increasing hypothesis space, high-order polynomials) we get excellent fit over the training data, but a poor representation of the true function: \textbf{overfitting}. While with low-order polynomials we have \textbf{under-fitting}. We want a \textbf{good generalization}.
    \begin{itemize}
        \item Applied to \textbf{linear models}: ridge regression and lasso, to determine $\lambda$ cannot use train error
        \item Such methods \textbf{shrink} the parameters towards \textbf{zero}
        \item \textbf{Regularization reduces variance, but increases bias}, parameter shrinkage method, a way to reduce the mean square error MSE
        \item \textbf{Increasing $\lambda\Rightarrow$increases training error$\Rightarrow$reduces variance}
        \item Ridge close-form solution
        \item \textbf{Lasso will put some features/noise to zero$\Rightarrow$when many features useless, lasso is better}
    \end{itemize}
    \begin{enumerate}
        \item \textbf{Ridge regression}:
        Change the loss function:
        $$L(w)=L_D(w)+\lambda L_W(w)$$
        $L_D(w)$ error on data (e.g. RSS)\\
        $L_W(w)$ model complexity\\
        Add a penalizing term for high weights (high $w$ means high variance, overfitting):
        $$L(w)=\frac{1}{2}\sum_{i=1}^N(t_i-w^T\phi(x_i))^2+\frac{\lambda}{2}||w||_2^2$$
        $$L(w)=\frac{1}{2}RSS(w)+\frac{\lambda}{2}||w||_2^2$$
        $$\hat{w}_{ridge}=(\lambda I+\Phi^T\Phi)^{-1}\Phi^Tt$$
        \begin{itemize}
            \item \textbf{Every eigenvalue must be greater than $\lambda$} as with ordinary least squares we had
            $$\hat{w}_{OLS}=(\Phi^T\Phi)^{-1}\Phi^Tt$$
            Now we add an eye matrix multiplied by $\lambda$ so the new eigenvalues are greater
            \item \textbf{Does not employ feature selection}
            \item \textbf{Increasing $\lambda$} means we are \textbf{regularizing more$\Rightarrow$simpler models$\Rightarrow$increases training error$\Rightarrow$reduces variance$\Rightarrow$bias increases}, also the weights tend/shrink to zero
            \item \textbf{Increasing $\lambda\Rightarrow$The test RSS decreases then increases}, at first we are less likely to overfit, but eventually model gets too simple a can't capture the true effects so test RSS will increase
            \item \textbf{Increasing $\lambda\Rightarrow$irreducible error remains constant}
            \item \textbf{Can be both parametric and non-parametric}
        \end{itemize}
        \item \textbf{Lasso}:
        Instead of L2-norm, we use L1-norm
        $$L(w)=\frac{1}{2}\sum_{i=1}^N(t_i-w^T\phi(x_i))^2+\frac{\lambda}{2}||w||_1$$
        $$L(w)=\frac{1}{2}RSS(w)+\frac{\lambda}{2}||w||_1$$
        $$||w||_1=\sum_{j=1}^M|w_j|$$
        \begin{itemize}
            \item Differently from ridge, lasso is \textbf{non-linear} and \textbf{no closed-form} solution exists, but the function is still \textbf{convex}
            \item Advantage respect to ridge: its non-linearity puts some weights to \textbf{zero, leading to sparser models (implicit feature selection)}. For values of $\lambda$ sufficiently large, removes some features, \textbf{includes in the model only meaningful inputs by doing variable selection}.\\
            Whereas Ridge will never allow weights to be 0. They might get very close to 0 but never 0 itself.
            \item Since we are reducing parameters (reducing model complexity), we might \textbf{increase bias}
            \item \textbf{Implicit feature selection}, so it yields \textbf{sparse} models: sparse models are \textbf{generally easier to interpret}, as they provide clear distinction between those input in which are meaningful (with non-zero parameters) and those which are less relevant (zero parameters)
        \end{itemize}
        \item \textbf{Elastic net}
        $$L(w)=\frac{1}{2}RSS(w)+\frac{\lambda_1}{2}||w||_2^2+\frac{\lambda_2}{2}||w||_1$$
    \end{enumerate}
\subsubsection{Dimensionality reduction (PCA)}
    \textbf{Unsupervised learning (disregards target variables, only looking at input variables)}, the input variables are projected into a \textbf{lower-dimensional subspace}: while in feature selection y/n for each feature, now take original feature space and compress it $\Rightarrow$ \textbf{combine features together to create new ones, hoping to save some features}.\\
    \textbf{Transform} original features and then model is learned on the transformed variables.\\
    Principal component analysis (PCA):
    \begin{itemize}
        \item Given dataset, find directions along the which have the largest dispersion, \textbf{project onto the input subspace which accounts for most of the variance}
        \item Find $m$ directions/lines, then project all points onto them: new dataset obtained by compression and we have discarded dimensions with small variance (\textbf{where more variance we have more info})
        \item Mean center the data, translate the original data so that it has zero mean:
        $$\overline{x}=\frac{1}{N}\sum_{n=1}^Nx_n$$
        \item Compute covariance matrix $S$ (\#features\_or X \#features\_or)
        $$S=\frac{1}{N-1}\sum_{n=1}^N(x_n-\overline{x})(x_n-\overline{x})^T$$
        $$C=\tilde{X}^T\tilde{X}$$
        Where $\tilde{X}$ is the translation of the original data so they have zero mean.\\
        Calculate eigenvalues and eigenvectors
        \begin{itemize}
            \item Eigenvector $e_1$ with largest eigenvalue $\lambda_1$ is the \textbf{first} principal component PC, and so on with $e_k$ as $k^{th}$ PC
            \item \textbf{Negative eigenvalues not meaningful}
            \item $\lambda_k/\sum_i\lambda_i$ is the proportion of \textbf{variance} captured by $k^th$ PC, higher eigenvalues, higher variance
            \item Values of eigenvalues are linear combination of a new feature
        \end{itemize}
        \item PCs produce new features that are linear combination of PCs
        \item Projection of original data onto first $k$ PCs ($E_k=(e_1,\cdots,e_k)$ is the orthogonal base) gives a \textbf{reduced dimensionality representation} of the data
        $$X'=XE_k$$
        For example, if 100 features
        \begin{itemize}
            \item X=\#samples x \#features
            \item $E_k$=100 x 10 (reduction of dimensionality, 10 PCs)
            \item X'=\#samples x \#PCs, new dataset
        \end{itemize}
        \item Given a sample vector $\tilde{x}$, its transformed version $t$ can be computed using:
        $$t=\tilde{x}W$$
        Where $t_i$ is the $i$-th principal component
        \begin{itemize}
            \item Loadings: $W$ matrix of the weights (matrix of eigenvectors)
            \item Scores: $T$ transformation of the input dataset $\tilde{X}$
            \item Variance: $(\lambda_1,\cdots,\lambda_M)$ vector of the variance of principal components
        \end{itemize}
        \item \textbf{Reconstruction} of the original data, transformed reduced dimensionality projection back into original space, but will have \textbf{some small acceptable error}.\\
        Given scores $t_i$ and the loadings $W$, we can reconstruct $x$ by applying again the loadings matrix $W$ to the scores $t_i$ thanks to the orthogonality property of $W$.\\
        \textbf{In general the reconstruction error will be greater than 0, so having exact original data not possible}.
        \item \textbf{Criterion to select number of PCs}:
        \begin{itemize}
            \item When sum of considered eigenvalues is 90\% of the total sum of eigenvalues, we fix a threshold
            \item Keep all PCs until we have a cumulative variance of 90\%-95\%
            \item Keep all PCs which have more than 5\% of variance (discard those with low variance)
            \item Find the elbow in the cumulative variance, if there is one the inclusion of the following PCs would not improve the representation of the data too much
        \end{itemize}
        \item \textbf{The more PCs, more accuracy, less reconstruction error}
        \item \textbf{No source of randomization, no initialization point in the algorithm to perform PCA}
        \item \textbf{Advantages}:
        \begin{itemize}
            \item Help reduce \textbf{computational complexity}
            \item Can help supervised learning, reduced dimension means simple hypothesis space and less risk of overfitting
            \item \textbf{Noise reduction}
        \end{itemize}
        \item \textbf{Disadvantages}:
        \begin{itemize}
            \item Fails when data consists of \textbf{multiple clusters}
            \item Direction of greatest variance may not be most informative
            \item Computation problems with many dimensions
            \item PCA computes \textbf{linear} combination of features, but data often lies on a \textbf{non-linear} manifold
        \end{itemize}
    \end{itemize}

\subsection{Model Ensemble}
    \textbf{The methods seen so far can reduce bias by increasing variance or vice versa}. With model ensemble:
    \begin{itemize}
        \item \textbf{Bagging}: reduce variance without increasing bias
        \item \textbf{Boosting}: reduce also the bias
    \end{itemize}
    They are meta-algorithms, \textbf{instead of learning one model, learn several and combine them.}\\
    Typically \textbf{improves accuracy, often by a lot}.\\
    \textbf{The two methods are not compatible, one or the other}.
    \begin{tabularx}{\linewidth}{p{0.2\textwidth}|X|X}
        \toprule
        \textbf{} & \textbf{Bagging (or bootstrapping)} & \textbf{Boosting}\\
        \midrule
        \endfirsthead
        \toprule
        \textbf{} & \textbf{Bagging (or bootstrapping)} & \textbf{Boosting}\\
        \\
        \midrule
        \endhead
        \midrule
        \footnotesize [Continues on next page]
        \endfoot
        \bottomrule
        \endlastfoot
        \textbf{Effect} & \textbf{Reduces variance} without increasing bias & \textbf{Reduces bias}\\ \midrule

        \textbf{On which model} & \textbf{Must have lot of variance}, model that overfits/unstable with \textbf{high dependency} on the training data & \textbf{Must have a lot of bias}, simple model \textbf{(weak learner)}\\ \midrule

        \textbf{When useless} & \textbf{Model with high bias}, model robust to change in the training data & \textbf{Model with high variance}, boosting will increase the variance, model must have almost no variance, no overfitting\\ \midrule

        \textbf{How it works} & \textbf{Decreases variance by averaging}.\newline
        Train a lot of models/predictions (e.g. $N$), \textbf{average} them:
        \newline\newline $Var(\overline{x})=\frac{Var(x)}{N}$\newline\newline
        Average within infinite element is zero\newline
        Build a lot of models with high variance and averaging them? \textbf{Dataset problem, the formula holds if variables in $\overline{X}$ are independent, independent models, if not we get less information}.\newline
        Try to use as much data as possible, \textbf{bootstrap samples}, take random samples from original dataset, random sampling with replacement. & 
        Combine many weak learners into a strong learner.\newline\newline
        1) \textbf{Weight all train samples equally, train weak/simple learner on training set} (that must be better than random, error > 0.5)\newline\newline
        2) There will be some samples on which learner is good, others on which is bad. \textbf{Compute error on each sample of training set, increase weights on train cases where model gets wrong}\newline\newline
        3) \textbf{Train new model on re-weighted train set}, re-compute error, increase weights on cases model gets wrong... \textbf{repeat until tired}\newline\newline
        Final model: \textbf{weighted} prediction of each model
        \\ \midrule

        \textbf{Parallelism} & \textbf{Ideal} for parallel architecture, more learners, better (high computational complexity) & \textbf{Cannot} do parallel training, inherently sequential (though weak learners are faster)\\ \midrule

        \textbf{Stable models} & Does \textbf{not} work well & \textbf{Might} help\\ \midrule

        \textbf{On noisy dataset} & No problem & Might \textbf{hurt performance}\\ \midrule

        \textbf{Helpfulness} & \textbf{In practice} almost always \textbf{helps} & \textbf{On average} helps \textbf{more} than bagging, but \textbf{also more common} for boosting to \textbf{hurt performance}\\ \midrule

        \textbf{Weights} & \textbf{Grows exponentially} & \textbf{Grows exponentially}

    \end{tabularx}